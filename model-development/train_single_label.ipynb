{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook documents the procedure for training the single-label models during the 2023 TRAM effort.\n",
    "\n",
    "The `bootstrap-training-data` file contains the annotations that existed prior, as well as the annotations that were produced during the 2023 effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>network traffic communicates over a raw socket.</td>\n",
       "      <td>T1095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>has the ability to set file attributes to hidden.</td>\n",
       "      <td>T1564.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>searches for files that are 60mb and less and ...</td>\n",
       "      <td>T1083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Attackers can use legitimate domains that are ...</td>\n",
       "      <td>T1090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>has registered two registry keys for shim data...</td>\n",
       "      <td>T1112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11125</th>\n",
       "      <td>usually the encrypted payload</td>\n",
       "      <td>T1027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11126</th>\n",
       "      <td>includes a capability to modify the Beacon pay...</td>\n",
       "      <td>T1027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11127</th>\n",
       "      <td>has collected the username of the victim system.</td>\n",
       "      <td>T1033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11128</th>\n",
       "      <td>has given malware the same name as an existing...</td>\n",
       "      <td>T1036.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11129</th>\n",
       "      <td>After the strings are decrypted</td>\n",
       "      <td>T1140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11130 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text      label\n",
       "0        network traffic communicates over a raw socket.      T1095\n",
       "1      has the ability to set file attributes to hidden.  T1564.001\n",
       "2      searches for files that are 60mb and less and ...      T1083\n",
       "3      Attackers can use legitimate domains that are ...      T1090\n",
       "4      has registered two registry keys for shim data...      T1112\n",
       "...                                                  ...        ...\n",
       "11125                      usually the encrypted payload      T1027\n",
       "11126  includes a capability to modify the Beacon pay...      T1027\n",
       "11127   has collected the username of the victim system.      T1033\n",
       "11128  has given malware the same name as an existing...  T1036.005\n",
       "11129                    After the strings are decrypted      T1140\n",
       "\n",
       "[11130 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "with open('../data/training/bootstrap-training-data.json') as f:\n",
    "    data_json = json.loads(f.read())\n",
    "\n",
    "data = pd.DataFrame(\n",
    "    [\n",
    "        {'text': row['text'], 'label': row['mappings'][0]['attack_id']}\n",
    "        for row in data_json['sentences']\n",
    "        if len(row['mappings']) > 0\n",
    "    ]\n",
    ")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load the model and move it to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "mode: 'bert or gpt' = 'bert'\n",
    "cuda = torch.device('cuda')\n",
    "\n",
    "if mode == 'bert':\n",
    "    model = transformers.BertForSequenceClassification.from_pretrained(\n",
    "        \"allenai/scibert_scivocab_uncased\",\n",
    "        num_labels=data['label'].nunique(),\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "    )\n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\", max_length=512)\n",
    "elif mode == 'gpt':\n",
    "    model = transformers.GPT2ForSequenceClassification.from_pretrained(\n",
    "        \"gpt2\",\n",
    "        num_labels=data['label'].nunique(),\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "    )\n",
    "    tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"gpt2\", max_length=512)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "else:\n",
    "    raise ValueError(f\"mode must be one of bert or gpt, but is {mode = !r}\")\n",
    "\n",
    "model.train().to(cuda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will represent the labels using one hot encoding.\n",
    "\n",
    "The `apply_attention_mask` function returns an attention mask (which is a tensor) where the element for every non-padding token is `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder as OHE\n",
    "\n",
    "encoder = OHE(sparse_output=False)\n",
    "encoder.fit(data[['label']])\n",
    "\n",
    "def tokenize(samples: 'list[str]'):\n",
    "    return tokenizer(samples, return_tensors='pt', padding='max_length', truncation=True, max_length=512).input_ids\n",
    "\n",
    "def load_data(x, y, batch_size=10):\n",
    "    x_len, y_len = x.shape[0], y.shape[0]\n",
    "    assert x_len == y_len\n",
    "    for i in range(0, x_len, batch_size):\n",
    "        slc = slice(i, i + batch_size)\n",
    "        yield x[slc].to(cuda), y[slc].to(cuda)\n",
    "\n",
    "def apply_attention_mask(x):\n",
    "    return x.ne(tokenizer.pad_token_id).to(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  102,   434,   501,  ...,     0,     0,     0],\n",
       "        [  102,  4199, 14562,  ...,     0,     0,     0],\n",
       "        [  102,   147,  3901,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  102, 10740,   111,  ...,     0,     0,     0],\n",
       "        [  102,   434, 13037,  ...,     0,     0,     0],\n",
       "        [  102,   121,   993,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size=.2, stratify=data['label'])\n",
    "\n",
    "x_train = tokenize(train['text'].tolist())\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = torch.Tensor(encoder.transform(train[['label']]))\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1298it [06:46,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss: 0.1005153461690929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1298it [06:52,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 loss: 0.03387916465871071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1298it [06:52,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 loss: 0.018095097056832536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "for epoch in range(6):\n",
    "    epoch_losses = []\n",
    "    for x, y in tqdm(load_data(x_train, y_train, batch_size=10)):\n",
    "        model.zero_grad()\n",
    "        out = model(x, attention_mask=apply_attention_mask(x), labels=y)\n",
    "        epoch_losses.append(out.loss.item())\n",
    "        out.loss.backward()\n",
    "        optim.step()\n",
    "    print(f\"epoch {epoch + 1} loss: {mean(epoch_losses)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['T1074.001', 'T1083', 'T1106', ..., 'T1053.005', 'T1566.001',\n",
       "       'T1082'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "model.eval()\n",
    "\n",
    "preds = []\n",
    "batch_size = 20\n",
    "\n",
    "x_test = tokenize(test['text'].tolist())\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, x_test.shape[0], batch_size):\n",
    "        x = x_test[i : i + batch_size].to(cuda)\n",
    "        out = model(x, attention_mask=apply_attention_mask(x))\n",
    "        preds.extend(out.logits.to('cpu'))\n",
    "\n",
    "predicted_labels = (\n",
    "    encoder.inverse_transform(\n",
    "        F.one_hot(\n",
    "            torch.vstack(preds).softmax(-1).argmax(-1),\n",
    "            num_classes=len(ALL_CLASSES)\n",
    "        )\n",
    "        .numpy()\n",
    "    )\n",
    "    .reshape(-1)\n",
    ")\n",
    "\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/TRAM2023/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/projects/TRAM2023/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P</th>\n",
       "      <th>R</th>\n",
       "      <th>F1</th>\n",
       "      <th>#</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>T1003.001</th>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1005</th>\n",
       "      <td>0.851064</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.776699</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1012</th>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1016</th>\n",
       "      <td>0.763441</td>\n",
       "      <td>0.910256</td>\n",
       "      <td>0.830409</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1021.001</th>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.936170</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1027</th>\n",
       "      <td>0.907514</td>\n",
       "      <td>0.907514</td>\n",
       "      <td>0.907514</td>\n",
       "      <td>346.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1033</th>\n",
       "      <td>0.760563</td>\n",
       "      <td>0.885246</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1036.005</th>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1041</th>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.685185</td>\n",
       "      <td>0.795699</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1047</th>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.932039</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1053.005</th>\n",
       "      <td>0.926471</td>\n",
       "      <td>0.969231</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1055</th>\n",
       "      <td>0.876712</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.920863</td>\n",
       "      <td>132.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1056.001</th>\n",
       "      <td>0.960784</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1057</th>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.931677</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1059.003</th>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.916230</td>\n",
       "      <td>0.938338</td>\n",
       "      <td>191.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1068</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1070.004</th>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.896970</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1071.001</th>\n",
       "      <td>0.902655</td>\n",
       "      <td>0.927273</td>\n",
       "      <td>0.914798</td>\n",
       "      <td>110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1072</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1074.001</th>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.723404</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1078</th>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.911392</td>\n",
       "      <td>0.905660</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1082</th>\n",
       "      <td>0.820896</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.849421</td>\n",
       "      <td>125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1083</th>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.836735</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1090</th>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1095</th>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1105</th>\n",
       "      <td>0.850242</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.900256</td>\n",
       "      <td>184.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1106</th>\n",
       "      <td>0.871287</td>\n",
       "      <td>0.897959</td>\n",
       "      <td>0.884422</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1110</th>\n",
       "      <td>0.945946</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1112</th>\n",
       "      <td>0.884058</td>\n",
       "      <td>0.938462</td>\n",
       "      <td>0.910448</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1113</th>\n",
       "      <td>0.978723</td>\n",
       "      <td>0.938776</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1140</th>\n",
       "      <td>0.948598</td>\n",
       "      <td>0.953052</td>\n",
       "      <td>0.950820</td>\n",
       "      <td>213.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1190</th>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.721311</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1204.002</th>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1210</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1218.011</th>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1219</th>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1484.001</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1518.001</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1543.003</th>\n",
       "      <td>0.736842</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1547.001</th>\n",
       "      <td>0.970149</td>\n",
       "      <td>0.942029</td>\n",
       "      <td>0.955882</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1548.002</th>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1552.001</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1557.001</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1562.001</th>\n",
       "      <td>0.854545</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.886792</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1564.001</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1566.001</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1569.002</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1570</th>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.786885</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1573.001</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.686567</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1574.002</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.987952</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(micro)</th>\n",
       "      <td>0.889642</td>\n",
       "      <td>0.889642</td>\n",
       "      <td>0.889642</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(macro)</th>\n",
       "      <td>0.835845</td>\n",
       "      <td>0.799192</td>\n",
       "      <td>0.803937</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  P         R        F1      #\n",
       "T1003.001  0.960784  0.960784  0.960784   51.0\n",
       "T1005      0.851064  0.714286  0.776699   56.0\n",
       "T1012      0.952381  0.689655  0.800000   29.0\n",
       "T1016      0.763441  0.910256  0.830409   78.0\n",
       "T1021.001  0.916667  0.956522  0.936170   46.0\n",
       "T1027      0.907514  0.907514  0.907514  346.0\n",
       "T1033      0.760563  0.885246  0.818182   61.0\n",
       "T1036.005  0.823529  0.823529  0.823529   51.0\n",
       "T1041      0.948718  0.685185  0.795699   54.0\n",
       "T1047      0.979592  0.888889  0.932039   54.0\n",
       "T1053.005  0.926471  0.969231  0.947368   65.0\n",
       "T1055      0.876712  0.969697  0.920863  132.0\n",
       "T1056.001  0.960784  1.000000  0.980000   49.0\n",
       "T1057      0.925926  0.937500  0.931677   80.0\n",
       "T1059.003  0.961538  0.916230  0.938338  191.0\n",
       "T1068      1.000000  0.428571  0.600000   14.0\n",
       "T1070.004  0.891566  0.902439  0.896970   82.0\n",
       "T1071.001  0.902655  0.927273  0.914798  110.0\n",
       "T1072      0.000000  0.000000  0.000000    4.0\n",
       "T1074.001  0.809524  0.653846  0.723404   26.0\n",
       "T1078      0.900000  0.911392  0.905660   79.0\n",
       "T1082      0.820896  0.880000  0.849421  125.0\n",
       "T1083      0.891304  0.836735  0.863158   98.0\n",
       "T1090      0.847059  0.960000  0.900000   75.0\n",
       "T1095      0.840000  0.840000  0.840000   25.0\n",
       "T1105      0.850242  0.956522  0.900256  184.0\n",
       "T1106      0.871287  0.897959  0.884422   98.0\n",
       "T1110      0.945946  1.000000  0.972222   35.0\n",
       "T1112      0.884058  0.938462  0.910448   65.0\n",
       "T1113      0.978723  0.938776  0.958333   49.0\n",
       "T1140      0.948598  0.953052  0.950820  213.0\n",
       "T1190      0.628571  0.846154  0.721311   26.0\n",
       "T1204.002  0.852941  0.966667  0.906250   60.0\n",
       "T1210      0.000000  0.000000  0.000000   11.0\n",
       "T1218.011  0.972973  0.972973  0.972973   37.0\n",
       "T1219      0.851852  0.920000  0.884615   25.0\n",
       "T1484.001  1.000000  0.900000  0.947368   10.0\n",
       "T1518.001  0.857143  0.769231  0.810811   39.0\n",
       "T1543.003  0.736842  1.000000  0.848485   42.0\n",
       "T1547.001  0.970149  0.942029  0.955882   69.0\n",
       "T1548.002  0.909091  0.952381  0.930233   21.0\n",
       "T1552.001  0.700000  0.538462  0.608696   13.0\n",
       "T1557.001  0.000000  0.000000  0.000000    4.0\n",
       "T1562.001  0.854545  0.921569  0.886792   51.0\n",
       "T1564.001  0.833333  0.625000  0.714286   16.0\n",
       "T1566.001  1.000000  0.866667  0.928571   60.0\n",
       "T1569.002  1.000000  0.142857  0.250000   21.0\n",
       "T1570      0.727273  0.857143  0.786885   28.0\n",
       "T1573.001  1.000000  0.522727  0.686567   44.0\n",
       "T1574.002  1.000000  0.976190  0.987952   42.0\n",
       "(micro)    0.889642  0.889642  0.889642    NaN\n",
       "(macro)    0.835845  0.799192  0.803937    NaN"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as calculate_score\n",
    "\n",
    "predicted = list(predicted_labels)\n",
    "actual = test['label'].tolist()\n",
    "\n",
    "labels = sorted(data['label'].unique())\n",
    "\n",
    "scores = calculate_score(actual, predicted, labels=labels)\n",
    "\n",
    "scores_df = pd.DataFrame(scores).T\n",
    "scores_df.columns = ['P', 'R', 'F1', '#']\n",
    "scores_df.index = labels\n",
    "scores_df.loc['(micro)'] = calculate_score(actual, predicted, average='micro', labels=labels)\n",
    "scores_df.loc['(macro)'] = calculate_score(actual, predicted, average='macro', labels=labels)\n",
    "\n",
    "scores_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
